# 🔹과제 1) 활성 함수의 종류를 설명한 것 (시그모이드, tanh, ReLU) 외에 아래 보기 중 선택하여 두 가지를 조사해오기

조사 내용으로는 식, 함수 모양, 그 활성함수의 장단점 각각 2가지 이상

---
#### 1. GELU (Gaussian Error Linear Unit)

<img width="701" height="432" alt="image" src="https://github.com/user-attachments/assets/41d58407-b0a2-437b-a51c-648bb00146f4" />

$$
	{GELU}(x) = x \cdot \Phi(x)
$$

또는 근사식으로:

$$
	{GELU}(x) \approx 0.5x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right)\right)
$$

- $\Phi(x)$는 표준 정규 분포의 누적 분포 함수(CDF)
- ReLU처럼 양수만 통과시키는 것이 아니라 **입력값에 따라 확률적으로 선택적으로 통과**
- 최근에는 BERT, GPT 등 트랜스포머 계열에서 널리 사용

**장점**:
- 입력을 부드럽게 반영함 → 학습 안정성 증가
- ReLU보다 성능이 더 좋다고 알려진 사례 있음

**단점**:
- 계산량이 tanh 포함 등으로 더 많음 → 연산 비용 증가
- 직관적 해석이 ReLU보다 어려움

--- 

- #### 2. Softmax

<img width="645" height="571" alt="image" src="https://github.com/user-attachments/assets/e58fd965-b561-4a49-a781-9812c46fbd5f" />

$$
	{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

- 입력 벡터 $\vec{z}$를 받아 **총합이 1이 되는 확률 벡터**로 변환
- 주로 **분류 문제의 출력층**에서 사용됨

**장점**:
- **출력을 확률로 해석**할 수 있어 다중 클래스 분류에 적합
- 모든 클래스에 대한 상대적인 중요도를 반영함

**단점**:
- 출력 간 상호의존성 존재 → 하나가 바뀌면 다른 출력도 바뀜
- 큰 값에 민감하여 **수치적 안정성 이슈** 발생 가능 (log-sum-exp trick 필요)

---

- #### 3. SoftPlus

<img width="833" height="653" alt="image" src="https://github.com/user-attachments/assets/f1380be0-4c2d-4a36-9c78-93b6072eccd1" />

$$
	{SoftPlus}(x) = \ln(1 + e^x)
$$

- ReLU 함수의 **부드러운 근사(smooth approximation)**
- $x$가 작을수록 0에 가까워지고, $x$가 클수록 $x$에 가까워짐
- 항상 미분 가능하며, 미분하면 sigmoid 함수가 됨

$$
\frac{d}{dx} \text{SoftPlus}(x) = \sigma(x)
$$

**장점**:
- 항상 양의 기울기를 가지므로 **죽은 뉴런 문제 없음**
- 부드러운 함수로 역전파가 더 안정적임

**단점**:
- ReLU보다 계산량이 많음 (로그, 지수 포함)
- 0 근처에서는 출력이 작아 학습이 느릴 수 있음

---

- #### 4. Leaky ReLU (Leaky Rectified Linear Unit)

<img width="838" height="637" alt="image" src="https://github.com/user-attachments/assets/741db624-50c5-45b9-8916-29a1a26492f2" />

$$
	{LeakyReLU}(x) =
\begin{cases}
x & \text{if } x \ge 0 \\
\alpha x & \text{if } x < 0
\end{cases}
$$

- $\alpha$ 는 작은 양수 (보통 0.01)
- ReLU와 유사하지만, **음수 입력에 대해 작은 기울기를 갖도록 함**

**장점**:
- **죽은 뉴런 문제를 완화**할 수 있음 (기울기가 0이 아님)
- 단순하고 계산이 빠름

**단점**:
-  $\alpha$ 값을 수동으로 설정해야 함 → 최적값 찾기 어려움
- 출력값의 평균이 0이 아니라서 일부 모델에서는 정규화 필요

---

- #### 5. Gaussian 함수

<img width="651" height="508" alt="image" src="https://github.com/user-attachments/assets/272b3327-23c8-447f-b5bc-a530176e8d7e" />

$$
f(x) = e^{-x^2}
$$

- 입력값이 0에 가까울수록 출력이 1에 가까워지고,  
  입력값이 멀어질수록 출력이 급격히 0으로 감소
- 주로 **RBF (Radial Basis Function)** 네트워크에서 사용됨

**장점**:
- 중심값 주변에 민감하게 반응함 → **국소적 특성(locality)** 보존
- 이상치에 둔감함

**단점**:
- 비선형 특성이 강해 **학습이 어려울 수 있음**
- 출력 범위가 (0, 1)로 제한되어 **기울기 소멸 문제**가 발생할 수 있음

---
# 🔹과제 2) 배치 경사하강법과 미니배치 경사하강법의 알고리즘에 대해 조사

간단하게 알고리즘의 단계와 해당 단계에서의 설명을 간단하게 작성

---
#### 1. 배치 경사하강법 (Batch Gradient Descent, BGD)

- 개념 설명
	- 배치 경사하강법은 전체 훈련 데이터셋을 사용하여 **한 번의 기울기 계산 후 가중치를 업데이트**하는 방식이다.
	- 매 반복(epoch)마다 전체 데이터를 사용하므로, **기울기가 정확하지만 느리다**.
	- 데이터가 클수록 계산 비용이 많이 든다.

-  알고리즘 순서

1. **모델 초기화**  
   - 가중치(weight)와 바이어스(bias)를 초기값으로 설정한다.

2. **전체 데이터에 대한 예측 수행**  
   - 모든 훈련 샘플에 대해 모델의 출력값을 계산한다.

3. **손실 함수 계산**  
   - 예측값과 실제값 사이의 오차를 전체 데이터 기준으로 계산한다.

4. **기울기(gradient) 계산**  
   - 전체 데이터를 기준으로 손실 함수의 기울기를 계산한다.

5. **가중치 업데이트**  
   - 계산된 기울기를 이용해 파라미터를 한 번 업데이트한다.
   - 학습률(learning rate)을 곱해서 이동 크기를 조절한다.

6. **수렴할 때까지 반복**  
   - 손실이 충분히 작아질 때까지 2~5번 과정을 반복한다.

---
#### 2. 미니배치 경사하강법 (Mini-Batch Gradient Descent)

- 개념 설명
	- 미니배치 경사하강법은 전체 훈련 데이터를 **작은 조각(mini-batch)** 으로 나누어 각 미니배치마다 기울기를 계산하고 가중치를 업데이트하는 방식이다.
	- 전체 데이터와 확률적 방법의 절충안이며, **연산 효율성과 학습 안정성**을 모두 잡을 수 있다.

 - 알고리즘 순서

1. **모델 초기화**  
   - 가중치(weight)와 바이어스(bias)를 초기값으로 설정한다.

2. **훈련 데이터를 미니배치로 분할**  
   - 지정한 크기만큼 데이터를 나누어 각 배치를 만든다.

3. **각 미니배치에 대해 다음을 반복**  
   - 예측 수행: 해당 배치의 입력값으로 모델 예측을 수행한다.  
   - 손실 계산: 예측값과 실제값 사이의 오차를 계산한다.  
   - 기울기 계산: 현재 배치에 대한 손실 함수의 기울기를 계산한다.  
   - 파라미터 업데이트: 계산된 기울기를 이용해 가중치를 업데이트한다.

4. **모든 배치를 순회한 후 다음 epoch으로 이동**  
   - 전체 배치를 모두 처리한 후 2~3 과정을 반복한다.

5. **수렴할 때까지 반복**  
   - 손실이 충분히 작아질 때까지 전체 epoch를 반복한다.
