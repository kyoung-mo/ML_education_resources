# ğŸ“˜ Week4â€‘1 â€“ ì†ì‹¤ í•¨ìˆ˜ & ì—­ì „íŒŒ

---

## ğŸ“‰ ì†ì‹¤ í•¨ìˆ˜ *(Loss Function)*

### ğŸ” `y`Â·`Å·`(yâ€‘hat) ê¸°í˜¸ ì´í•´
| ê¸°í˜¸ | ì½ëŠ” ë²• | ì˜ë¯¸ |
|------|--------|------|
| **`y`** | ì™€ì´ | **ì‹¤ì œê°’**Â·ì •ë‹µ(Ground Truth) |
| **`Å·`** (yâ€‘hat) | ì™€ì´â€‘í–‡ | **ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’**Â·ì¶”ì •ì¹˜(Prediction) |

> ëª¨ì(^) ëª¨ì–‘ **â€œhatâ€** ì€ í†µê³„Â·ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ â€œì¶”ì •(estimation)â€ì„ ë‚˜íƒ€ë‚´ëŠ” ê´€ë¡€ì  í‘œê¸°ì…ë‹ˆë‹¤.  
> ì¦‰, ì†ì‹¤ í•¨ìˆ˜ëŠ” **ì‹¤ì œê°’ `y`** ì™€ **ì˜ˆì¸¡ê°’ `Å·`** ì˜ ì°¨ì´ë¥¼ ìˆ˜ì¹˜í™”í•©ë‹ˆë‹¤.

---

### 1ï¸âƒ£ ì†ì‹¤ í•¨ìˆ˜ ëª¨ìŒ

| ë²”ì£¼ | í•¨ìˆ˜ | ì‹ (ìš”ì•½) | íŠ¹ì§• / ì‚¬ìš©ì²˜ |
|------|------|-----------|----------------|
| íšŒê·€ | **MSE** | $\dfrac{1}{N} \sum (y - \hat{y})^2$ | ì´ìƒì¹˜ ë¯¼ê°, ë³´í¸ì  |
|      | MAE | $\dfrac{1}{N} \sum \lvert y - \hat{y} \rvert$ | ì´ìƒì¹˜ ê°•ì¸ |
| ì´ì§„ ë¶„ë¥˜ | **BCE** | $- \big[ y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \big]$ | ì‹œê·¸ëª¨ì´ë“œ ì´í›„ ì‚¬ìš© |
| ë‹¤ì¤‘ ë¶„ë¥˜ | **Cross-Entropy** | $- \sum y_k \log \hat{p}_k$ | ì†Œí”„íŠ¸ë§¥ìŠ¤ ì´í›„ |
| ë¶ˆê· í˜• | Focal Loss | $- (1 - \hat{p})^{\gamma} \log \hat{p}$ | ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ |
| ë¶„í¬ ê±°ë¦¬ | KL Divergence | $\sum p \log \frac{p}{q}$ | ì§€ì‹ ì¦ë¥˜, VAE |

> ì—¬ê¸°ì„œ $\hat{p}$ ì—­ì‹œ **ì˜ˆì¸¡ í™•ë¥ **ì„ ëœ»í•©ë‹ˆë‹¤.

---

### 2ï¸âƒ£ PyTorch ì˜ˆì œ

```python
import torch
import torch.nn as nn

logits  = torch.tensor([[2.0, 0.5, -1.0]])  # ëª¨ë¸ì´ ë‚¸ ì ìˆ˜(logit)
targets = torch.tensor([0])                 # ì •ë‹µ ë ˆì´ë¸”

loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(logits, targets)
print("Crossâ€‘Entropy Loss:", loss.item())
```

---

## ğŸ” ì—­ì „íŒŒ *(Backpropagation)*

### 1ï¸âƒ£ ê°œë…
- ì†ì‹¤ì´ ì¤„ì–´ë“¤ë„ë¡ **íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜Â·í¸í–¥)** ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜  
- **ì²´ì¸ ë£°**ë¡œ ë‹¤ì¸µ ë¯¸ë¶„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°

### 2ï¸âƒ£ PyTorch í•™ìŠµ ì ˆì°¨

| ë‹¨ê³„ | ì½”ë“œ | ì—­í•  |
|------|------|------|
| ìˆœì „íŒŒ | `y_pred = model(x)` | ê·¸ë˜í”„ ìƒì„± |
| ì†ì‹¤ ê³„ì‚° | `loss = criterion(y_pred, y)` | ë…¸ë“œ ì¶”ê°€ |
| ì—­ì „íŒŒ | `loss.backward()` | âˆ‚L/âˆ‚Î¸ ê³„ì‚° |
| ì—…ë°ì´íŠ¸ | `optimizer.step()` | Î¸ â† Î¸Â âˆ’Â Î·Â·âˆ‚L/âˆ‚Î¸ |
| ì´ˆê¸°í™” | `optimizer.zero_grad()` | ê·¸ë˜ë””ì–¸íŠ¸ ë¦¬ì…‹ |

---

## ğŸ”¬ [ì‹¤ìŠµ] ë„˜íŒŒì´ vs íŒŒì´í† ì¹˜ë¡œ 2ë‹¨ MLP â€œì—­ì „íŒŒ(gradient)â€ ë¹„êµ

### 1ï¸âƒ£ ë„˜íŒŒì´ë¡œ (ìˆ˜ë™ ë¯¸ë¶„)

```python
import numpy as np

def sigmoid(x): return 1 / (1 + np.exp(-x))
def d_sigmoid(x): return sigmoid(x) * (1 - sigmoid(x))

x = np.array([1.0, 0.5])
y_true = np.array([1.0])

W1 = np.array([[0.1, 0.2], [0.3, 0.4]])
b1 = np.array([0.1, 0.2])
W2 = np.array([[0.5], [0.6]])
b2 = np.array([0.3])

z1 = np.dot(x, W1) + b1
a1 = sigmoid(z1)
z2 = np.dot(a1, W2) + b2
a2 = sigmoid(z2)
loss = 0.5 * np.sum((a2 - y_true)**2)

d_loss_a2 = a2 - y_true
d_loss_z2 = d_loss_a2 * d_sigmoid(z2)
d_loss_W2 = np.outer(a1, d_loss_z2)
d_loss_b2 = d_loss_z2

d_loss_a1 = np.dot(W2, d_loss_z2)
d_loss_z1 = d_loss_a1 * d_sigmoid(z1)
d_loss_W1 = np.outer(x, d_loss_z1)
d_loss_b1 = d_loss_z1

print("NumPy dW1:", d_loss_W1)
print("NumPy dW2:", d_loss_W2)
```

### ğŸ§ª ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê²€ì¦

```python
def numerical_gradient(f, W, h=1e-5):
    grad = np.zeros_like(W)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            tmp = W[i, j]
            W[i, j] = tmp + h
            fxh1 = f()
            W[i, j] = tmp
            fx = f()
            grad[i, j] = (fxh1 - fx) / h
    return grad

def loss_func_w1():
    z1 = np.dot(x, W1) + b1
    a1 = sigmoid(z1)
    z2 = np.dot(a1, W2) + b2
    a2 = sigmoid(z2)
    return 0.5 * np.sum((a2 - y_true)**2)

num_grad_W1 = numerical_gradient(loss_func_w1, W1)
print("ìˆ˜ì¹˜ ë¯¸ë¶„ dW1:", num_grad_W1)
print("í•´ì„ ë¯¸ë¶„ dW1:", d_loss_W1)
```

---

### 2ï¸âƒ£ íŒŒì´í† ì¹˜ë¡œ (ìë™ ë¯¸ë¶„)

```python
import torch

W1 = torch.tensor([[0.1, 0.2], [0.3, 0.4]], requires_grad=True)
b1 = torch.tensor([0.1, 0.2], requires_grad=True)
W2 = torch.tensor([[0.5], [0.6]], requires_grad=True)
b2 = torch.tensor([0.3], requires_grad=True)
x = torch.tensor([1.0, 0.5])
y_true = torch.tensor([1.0])

def sigmoid(x): return 1 / (1 + torch.exp(-x))

z1 = torch.matmul(x, W1) + b1
a1 = sigmoid(z1)
z2 = torch.matmul(a1, W2) + b2
a2 = sigmoid(z2)
loss = 0.5 * ((a2 - y_true) ** 2).sum()
loss.backward()

print("PyTorch dW1:", W1.grad)
print("PyTorch dW2:", W2.grad)
```

---

## ğŸ› ï¸ ì‹¤ìŠµ: ì„ í˜• íšŒê·€ë¡œ `y=3x` ê·¼ì‚¬

```python
import torch, torch.nn as nn, torch.optim as optim

model = nn.Linear(1, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.05)

x = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[3.0], [6.0], [9.0]])

for epoch in range(200):
    pred = model(x)
    loss = criterion(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 20 == 0:
        print(f"Epoch {epoch+1:3d}: Loss={loss.item():.4f}")

print("í•™ìŠµëœ w, b:", model.weight.item(), model.bias.item())
```

---

## ğŸ¯ ê³¼ì œ

1. ìœ„ ì˜ˆì œë¥¼ ì°¸ê³ í•´ `nn.Linear(1,1)` ë¡œ **`y = 3x`** ê·¼ì‚¬í•˜ê¸°  
2. `nn.MSELoss()` ì‚¬ìš©, **ì—í­ë§ˆë‹¤ ì†ì‹¤** ì¶œë ¥  
3. í•™ìŠµ ì¢…ë£Œ í›„ **weight(â‰ˆ3) & bias(â‰ˆ0)** í™•ì¸  
4. 2ë‹¨ MLP êµ¬ì¡°ë¡œ ìœ„ì™€ ê°™ì€ ë„˜íŒŒì´/íŒŒì´í† ì¹˜ ì—­ì „íŒŒ ì‹¤ìŠµ ì§ì ‘ ìˆ˜í–‰  
5. ì¶”ê°€ ì‹¤í—˜: **ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ í•´ì„ ë¯¸ë¶„ ê²°ê³¼ ê²€ì¦**

---

âœ… **í™˜ê²½**: PythonÂ 3.x, GoogleÂ Colab, numpy, PyTorch â‰¥Â 2.0  
ì„¤ì¹˜: `!pip install numpy torch torchvision -q`


---

