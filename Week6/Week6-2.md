## 1️⃣ RNN

---
### 🔹 RNN의 구조 및 상태(State)

RNN에서 **state**는 일종의 **기억(Memory)** 을 의미하며,  
입력 데이터를 시간 순서에 따라 처리하면서 **과거 정보를 저장하고 다음 시점에 전달**한다.

RNN은 일반적으로 다음의 **세 가지 상태**로 구성된다:

| 상태 종류                | 기호    | 역할                                               |
| -------------------- | ----- | ------------------------------------------------ |
| 입력 상태 (Input State)  | $x_t$ | 시점 $t$에서 들어오는 입력값                                |
| 은닉 상태 (Hidden State) | $h_t$ | 이전 시점의 은닉 상태 $h_{t-1}$와 입력 $x_t$를 바탕으로 계산된 기억 정보 |
| 출력 상태 (Output State) | $y_t$ | 은닉 상태를 바탕으로 계산된 출력값 (예: 예측 결과)                   |

$h_t = f(x_t, h_{t-1})$  
$y_t = g(h_t)$

이처럼 **입력, 은닉, 출력 상태**는 시간 축을 따라 반복적으로 연결되어  
RNN이 **시간적 의존성**을 학습할 수 있도록 돕는다.

---
### 🔹 시계열 데이터를 다루기 위한 새로운 접근

일반적인 딥러닝 모델(예: 완전연결 신경망, CNN)은 **입력 간의 순서나 시간 정보를 고려하지 않는다.**

하지만 실제로 우리가 처리하고자 하는 많은 데이터는 **시간적인 흐름이나 순서 정보**를 갖는다.

예를 들어,

- 문장: "나는 밥을 먹었다." 
- 음성: 시간에 따라 변화하는 파형 
- 주식 가격: 과거의 흐름에 따라 미래가 결정됨  
- 날씨: 이전 날짜의 정보가 중요한 영향을 끼침

이러한 데이터는 **이전의 정보(기억)** 를 바탕으로 현재의 출력이 결정되는 구조를 가지고 있다.

---
### 🔹 기존 신경망의 한계

완전연결 신경망(Dense Layer)을 이용해도 각 시점의 데이터를 독립적으로 처리할 수는 있다.

하지만 다음과 같은 문제점이 발생한다.

#### ❌ 시간 순서가 고려되지 않음
입력 데이터를 한 번에 flatten하거나, 같은 네트워크로 처리하면  
각 시점 사이의 연관성이나 순서 정보가 사라진다.

#### ❌ 과거 정보를 기억하지 못함
이전 시점의 정보가 현재의 예측에 반영되어야 하는 경우,  
기존 신경망은 매 시점의 입력만을 기반으로 예측하므로 **문맥이나 맥락이 반영되지 않는다.**

---
### 🔹 RNN의 핵심 아이디어

RNN은 **기억(Memory)** 이 있는 신경망이다.  
이전 시점의 출력을 **다음 시점의 입력으로 전달**하여 시계열적인 연산이 가능하도록 한다.

$h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$  
$y_t = W_{hy}h_t + b_y$

- $x_t$: 시점 $t$의 입력 (input at time step $t$)  
- $h_{t-1}$: 이전 시점의 은닉 상태 (메모리 역할)  
- $h_t$: 현재 시점의 은닉 상태 (업데이트된 메모리)  
- $y_t$: 현재 시점의 출력  
- $f$: 비선형 활성화 함수 (예: tanh, ReLU 등)

RNN은 **시간 축을 따라 연속적인 연산**을 수행하면서,  
입력의 순서를 고려하고 과거 정보를 저장할 수 있는 구조를 갖는다.

---
### 🔹 시각화

![[Pasted image 20250804113250.png]]

- 입력: $[x_{t-2}, x_{t-1}, x_t, \dots]$
- 순차적으로 hidden state ($h_t$) 를 계산하면서 정보 전달  
- 최종적으로는 각 시점의 출력 ($y_t$) 또는 전체 시퀀스에 대한 출력 가능

---
## 2️⃣ RNN의 기본 구조: 순환 연결과 Hidden State

---
### 🔹 순환(Recurrent) 구조란?

RNN의 핵심은 **출력값을 다시 자기 자신에게 입력으로 넣는**  
**순환 구조(recurrent connection)** 에 있다.

이 구조는 시간에 따라 변화하는 데이터를 처리하면서,  
과거의 정보를 현재까지 **연결해서 기억**할 수 있게 해준다.

---
### 🔹 기본적인 RNN 셀의 수식 구조

하나의 RNN 셀(Cell)은 아래와 같은 계산을 수행한다.

$$
h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
$$

$$
y_t = W_{hy}h_t + b_y
$$

- $x_t$: 시점 $t$의 입력  
- $h_{t-1}$: 이전 시점의 은닉 상태 (메모리 역할)  
- $h_t$: 현재 시점의 은닉 상태 (업데이트된 메모리)  
- $y_t$: 현재 시점의 출력  
- $\tanh$: 비선형 활성화 함수 (또는 ReLU 등)

---
### 🔹 시각화: 시간축으로 펼친 구조 (Unrolling)

RNN의 순환 구조는 시간에 따라 **반복적으로 연결**되는 형태이다.  
이를 **시간축으로 펼치면** 아래 그림처럼 **각 시점마다 동일한 연산이 반복되는 구조**로 표현할 수 있다.

![[Pasted image 20250804110120.png]]


- 모든 시점에서 **같은 가중치**(예: $W_{xh}, W_{hh}, W_{hy}$)를 공유하여 학습
- **이전 시점의 은닉 상태** $h_{t-1}$ 이 현재 시점의 은닉 상태 $h_t$ 계산에 영향을 줌
- 마지막 시점의 은닉 상태 $h_t$를 통해 최종 출력 $y_t$ 생성

이러한 구조 덕분에 RNN은 **시간적인 연속성**과 **문맥 정보**를 반영할 수 있다.


---
#### 🔹 Unrolling의 목적

- 시간적 흐름을 이해하기 쉽도록 시각화
- BPTT(시간 역전파)를 설명하기 위해 필수적인 개념
- 시퀀스 전체에 걸친 **의존 관계**를 추적할 수 있음

---
### 🔹 Hidden State의 역할

- $h_t$는 이전 상태와 현재 입력 정보를 반영하여 계산된 **기억 상태**이다.
- 이 값이 다음 시점으로 전달되며, **시계열 정보를 압축하여 보존**한다.
- 결과적으로 RNN은 입력의 순서를 인식하고, 그에 따라 **동적으로 반응**할 수 있다.

---
### 🔹 예시: 텍스트 처리

입력 시퀀스:  나는 / 밥을 / 먹었 / 다

출력 시퀀스:  밥을 / 먹었 / 다 / [EOS]


- EOS는 End Of Sequence의 약자로, 시퀀스의 끝을 나타내는 토큰
- RNN은 각 단어의 의미뿐 아니라 **앞에서 본 단어의 흐름(문맥)** 을 고려하여 처리할 수 있다.

---

| 개념        | 의미                        |
| --------- | ------------------------- |
| RNN 셀     | 하나의 순환 구조                 |
| Unfolding | 셀을 시간 순서대로 펼쳐서 나열         |
| 공유 가중치    | 모든 시점에서 동일한 가중치를 사용       |
| 목적        | 시간 흐름에 따른 계산과 역전파를 명확히 표현 |

---
## 3️⃣ 수식 기반 설명

---

RNN은 시계열 데이터를 처리하기 위해 **입력과 은닉 상태를 조합하여 새로운 은닉 상태를 계산**하고,  
이를 기반으로 출력을 만들어낸다.

이 과정은 아래와 같은 **두 가지 핵심 수식**으로 구성된다:

---

### 🔹 1. 은닉 상태 계산 (Hidden State)

$$
h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
$$

- $h_t$: 시점 $t$의 은닉 상태 (memory)  
- $x_t$: 시점 $t$의 입력  
- $h_{t-1}$: 이전 시점의 은닉 상태  
- $W_{xh}$: 입력에서 은닉으로 가는 가중치 행렬  
- $W_{hh}$: 은닉에서 은닉으로 전달되는 가중치 행렬  
- $b_h$: 은닉 상태의 편향  
- $\tanh$: 비선형 활성화 함수

이 수식은 **과거의 정보($h_{t-1}$)** 와 **현재 입력($x_t$)** 을  
선형 결합한 후, $\tanh$ 함수를 통과시켜 **현재의 상태($h_t$)** 를 구하는 방식이다.

즉, RNN은 **과거와 현재의 정보를 합성**하여 다음 상태를 만들어낸다.

---

### 🔹 2. 출력 계산 (Output)

$$
y_t = W_{hy}h_t + b_y
$$

- $y_t$: 시점 $t$의 출력값  
- $W_{hy}$: 은닉 상태에서 출력으로 가는 가중치  
- $b_y$: 출력층의 편향

이 수식은 은닉 상태 $h_t$를 선형 변환하여 출력 $y_t$를 구한다.  
분류 문제의 경우, 출력에 소프트맥스 함수를 적용하여 확률 분포로 변환할 수 있다.

---

### 🔹 계산 흐름 요약

입력 (xₜ) + 이전 상태 (hₜ₋₁)  
	↓  
선형 변환 + tanh  
	↓  
현재 상태 (hₜ)  
	↓  
선형 변환  
	↓  
출력 (yₜ)


---

### 🔹 은닉 상태의 시간 축 흐름

$$
h_0 \rightarrow h_1 \rightarrow h_2 \rightarrow \cdots \rightarrow h_t
$$

각 시점의 $h_t$는 이전 시점의 $h_{t-1}$에 의존하므로,  
**RNN은 과거 정보를 누적하여 처리하는 구조**를 갖는다.

이 구조 덕분에 RNN은 다음과 같은 문제를 다룰 수 있다.

- 문장의 문맥 파악
- 시계열 예측
- 연속적인 신호 처리 등

---
## 4️⃣ BPTT (Backpropagation Through Time)

---

### 🔹 RNN의 학습 과정

RNN은 일반적인 신경망처럼 **손실 함수의 값을 줄이기 위해**  
오차 역전파(Backpropagation)를 이용하여 가중치를 업데이트한다.

하지만 RNN은 **시간 축을 따라 연결된 순환 구조**를 가지고 있기 때문에,  
오차 역전파 또한 **시간에 따라 펼쳐진 전체 구조를 따라 전파**되어야 한다.

이 과정을 **시간을 통한 역전파**, 즉  
**BPTT (Backpropagation Through Time)** 라고 부른다.

---
### 🔹 시간 축을 펼친 구조에서의 역전파

RNN을 시점 $t=1$부터 $t=T$까지 펼친 후,  
출력 오차를 기준으로 **맨 끝 시점부터 역방향으로 기울기를 전파**한다.

$$
L = \sum_{t=1}^T \mathcal{L}(y_t, \hat{y}_t)
$$

- $L$: 전체 손실 함수 (전체 시퀀스에 대한 합)  
- $y_t$: 예측값  
- $\hat{y}_t$: 실제 정답  
- $\mathcal{L}$: 시점별 손실 함수 (예: 크로스엔트로피)

---
### 🔹 주요 특징

1. **가중치 공유**  
   - RNN은 모든 시점에서 동일한 가중치($W_{xh}, W_{hh}, W_{hy}$)를 사용함  
   - 따라서 각 시점에서의 기울기를 모두 **합산**하여 가중치를 업데이트해야 함

2. **시간 축을 따라 기울기가 누적**  
   - 역전파는 $h_T \rightarrow h_{T-1} \rightarrow \cdots \rightarrow h_1$ 방향으로 진행됨  
   - 각 시점에서의 은닉 상태는 다음 시점의 기울기에 영향을 받음

---
### 🔹 수식 예시

역전파는 체인룰(chain rule)을 통해 다음처럼 전개된다.

$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{hh}}
$$

여기서 중요한 점은  $\frac{\partial h_t}{\partial W_{hh}}$ 계산 시  
**$h_{t-1}, h_{t-2}, \dots$ 까지의 정보가 모두 연결되어 있다는 점**이다.

---

### 🔹 시각적 설명

x₁     →     x₂     →     x₃     → ...
 ↓           ↓           ↓
h₁  →     h₂  →     h₃  → ...
 ↓           ↓           ↓
y₁         y₂         y₃
 ↑           ↑           ↑
∂L/∂h₁    ∂L/∂h₂    ∂L/∂h₃


- 각 시점에서의 출력 오차가 은닉 상태의 기울기로 역전파됨  
- 이후 은닉 상태 간의 연결($W_{hh}$)을 따라 더 이전 시점으로 전파됨

1. **순방향 전달 (Forward)**
  - $x_1 \to h_1 \to y_1$
  - $x_2 \to h_2 \to y_2$
  - $x_3 \to h_3 \to y_3$  
    : 입력 $x_t$는 은닉 상태 $h_t$를 거쳐 출력 $y_t$를 생성
  
2. **손실 계산 (Loss)**
  - $y_t$와 실제 정답 $\hat{y}_t$를 비교하여 **손실 $L$** 계산
  - 이 손실에 따라 출력 $y_t$의 오차가 거꾸로 전파됨
   
3. **역전파 시작**
   - 출력 $y_t$의 오차가 은닉 상태 $h_t$로 역전파됨 → $\frac{\partial L}{\partial h_t}$
   - 즉, 출력 오차가 은닉 상태의 기울기로 전환됨

1. **시간을 따라 연결된 은닉 상태로 역전파**
   - $h_3 \to h_2 \to h_1$ 순서로,  
   - $W_{hh}$를 통해 **이전 시점의 은닉 상태로 계속 오차가 전달됨**

---
### 🔹 BPTT의 어려움: 기울기 소실과 폭주

시간이 길어질수록 다음과 같은 현상이 발생할 수 있다:

- **기울기 소실 (vanishing gradient)**  
  반복적인 곱셈 연산으로 인해 기울기가 매우 작아져 학습이 멈춤

- **기울기 폭주 (exploding gradient)**  
  반대로 기울기가 급격히 커져 학습이 불안정해짐

---
## 5️⃣ RNN의 한계

---
### 🔹 장기 의존성 문제 (Long-Term Dependency)

RNN은 시간에 따른 정보를 처리할 수 있는 구조를 가지고 있지만,  
**멀리 떨어진 과거 정보는 현재에 잘 전달되지 않는 한계**를 갖고 있다.

이 문제를 **장기 의존성 문제 (long-term dependency)** 라고 부른다.

#### 예시

입력: 오늘 / 날씨가 / 너무 / 좋아서 / 기분이 / 정말 / 좋았다 / 😊  
출력: 긍정

입력: 오늘 / 날씨가 / 너무 / 안좋고 / 계속 / 비가 / 와서 / 우울하다  
출력: 부정


이처럼 마지막 출력은 **앞의 정보(날씨)** 에 따라 달라져야 한다.  
하지만 RNN은 시점이 멀어질수록 앞의 정보 전달이 약해져 **의미 있는 연결을 유지하기 어렵다.**

---
### 🔹 수식적으로 본 기울기 전파

BPTT를 통해 가중치에 대한 기울기를 계산할 때,  
다수의 연쇄된 곱셈으로 인해 기울기가 아래처럼 표현된다.

$$
\frac{\partial L}{\partial W_{hh}} \propto \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}}
$$

은닉 상태는 아래와 같이 활성화 함수를 포함한 선형 결합이다.

$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
$$

그 결과, 역전파에서 다음과 같은 문제가 발생한다.

---

### 🔹 기울기 소실 (Vanishing Gradient)

- $\tanh$ 또는 $\text{sigmoid}$의 도함수는 항상 $< 1$
- 이를 시간만큼 계속 곱하면 **기울기가 지수적으로 감소**
- 결과적으로 초반 시점의 기울기가 **0에 수렴 → 학습이 안 됨**

$$
\left| \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}} \right| \rightarrow 0
$$

---

### 🔹 기울기 폭주 (Exploding Gradient)

- 반대로 가중치나 활성화 함수의 도함수가 $> 1$인 경우
- 기울기가 반복적으로 곱해지며 **급격히 커지는 현상**
- 학습이 발산하거나 NaN이 되는 등 불안정해짐

$$
\left| \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}} \right| \rightarrow \infty
$$

#### 대처법
- **기울기 클리핑 (gradient clipping)** 을 통해 폭주 방지 가능
- 하지만 **소실 문제는 구조적인 해결이 어려움**

---

### 🔹 RNN 구조의 근본적 한계

| 문제 | 영향 |
|------|------|
| 장기 의존성 | 과거 정보를 기억하지 못함 |
| 기울기 소실 | 멀리 있는 가중치는 업데이트되지 않음 |
| 기울기 폭주 | 수렴하지 않고 학습이 불안정함 |

---

### 🔹 해결 방법의 필요성

이러한 문제를 해결하기 위해 등장한 것이 바로  
**LSTM(Long Short-Term Memory)** 와 **GRU(Gated Recurrent Unit)** 이다.

이들은 게이트(Gate)를 사용하여 정보 흐름을 조절하고,  
장기 의존성 문제를 효과적으로 극복할 수 있도록 고안된 구조이다.

## 6️⃣ 간단한 구현 예제 (PyTorch 기반)

---
### 🔹 목표

- RNN의 동작 구조를 코드로 확인
- 입력 → 은닉 상태 → 출력 흐름을 직접 구현
- PyTorch에서 제공하는 RNN 모듈 사용법 익히기

---

### ✅ 예제 1: RNNCell 수동 구현 (한 시점)

```python
import torch
import torch.nn as nn

# 입력: 크기 4, 은닉 상태: 크기 3
x_t = torch.tensor([[0.5, 0.1, -0.3, 0.7]])  # (batch=1, input_size=4)
h_prev = torch.zeros(1, 3)  # (batch=1, hidden_size=3)

# 가중치 초기화
W_xh = torch.randn(4, 3) * 0.1
W_hh = torch.randn(3, 3) * 0.1
b_h = torch.zeros(1, 3)

# 수식 계산: h_t = tanh(x_t @ W_xh + h_{t-1} @ W_hh + b)
h_t = torch.tanh(x_t @ W_xh + h_prev @ W_hh + b_h)

print("은닉 상태 h_t:", h_t)
```

```python
import torch
import torch.nn as nn

# RNN 모델 정의
rnn = nn.RNN(input_size=4, hidden_size=3, batch_first=True)

# 입력 시퀀스 (batch=2, time_step=5, input_size=4)
x = torch.randn(2, 5, 4)  # 2개의 시퀀스, 각 시퀀스 길이 5

# 초기 은닉 상태 (num_layers=1, batch=2, hidden_size=3)
h0 = torch.zeros(1, 2, 3)

# 순전파 실행
out, hn = rnn(x, h0)

print("출력 시퀀스 out.shape:", out.shape)  # (2, 5, 3)
print("최종 은닉 상태 hn.shape:", hn.shape)  # (1, 2, 3)
```

### 🔹 구조 해석

`nn.RNN`은 내부적으로 시간축을 따라 `RNNCell`을 반복 수행한다.  
입력 시퀀스 $x = [x_1, x_2, \dots, x_T]$ 에 대해  
은닉 상태 $h_t$를 순서대로 업데이트하며 출력한다.

---
### 🔹 출력 설명

| 변수    | 의미            | 형태                                 |
| ----- | ------------- | ---------------------------------- |
| `x`   | 입력 시퀀스        | `(batch, time_step, input_size)`   |
| `out` | 모든 시점의 출력     | `(batch, time_step, hidden_size)`  |
| `hn`  | 마지막 시점의 은닉 상태 | `(num_layers, batch, hidden_size)` |

---
### 🔹 예제 활용 아이디어

- 임의의 수열 (예: `[1, 2, 3, 4]`)을 입력으로 주고  
  다음 값을 예측하도록 학습 구조 확장 가능  
- 문자 생성, 감정 분류, 주가 예측 등에도 확장 가능

---
### 🔹 정리

RNN은 한 시점씩 hidden state를 업데이트하는 구조로 작동한다.  
PyTorch에서는 `nn.RNN`, `nn.LSTM`, `nn.GRU` 등의 모듈을 사용하여  
복잡한 시계열 모델을 간단하게 구성할 수 있다.

이후 수업에서는 RNN의 구조적 한계를 극복한  
**LSTM과 GRU 구조**, 그리고 **시계열 예측 실습**으로 이어진다.

